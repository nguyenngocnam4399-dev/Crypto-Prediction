# üöÄ Deterministic Quantitative Trading Research Platform  
### H·ªá Th·ªëng Ph√¢n T√≠ch & D·ª± ƒêo√°n T√†i S·∫£n S·ªë D·ª±a Tr√™n D·ªØ Li·ªáu Th·ªùi Gian Th·ª±c  

**Author:** Nguy·ªÖn Ng·ªçc Nam  
**Mentor:** Ph·∫°m Long V√¢n - Data Manager  
**Location:** Ho Chi Minh City, Vietnam ‚Äî 2025  

---

# 1Ô∏è‚É£ Introduction

Trong b·ªëi c·∫£nh t√†i s·∫£n s·ªë (Crypto Assets) ng√†y c√†ng ph√°t tri·ªÉn m·∫°nh m·∫Ω, ƒë·∫∑c bi·ªát l√† c√°c ƒë·ªìng c√≥ t√≠nh thanh kho·∫£n cao nh∆∞ BTC, ETH, BNB, s·ªë l∆∞·ª£ng nh√† ƒë·∫ßu t∆∞ c√° nh√¢n v√† t·ªï ch·ª©c tham gia th·ªã tr∆∞·ªùng ng√†y c√†ng tƒÉng.
Song song v·ªõi ƒë√≥, xu h∆∞·ªõng qu·∫£n l√Ω v√† c·∫•p ph√©p t√†i s·∫£n s·ªë t·∫°i nhi·ªÅu qu·ªëc gia (bao g·ªìm Vi·ªát Nam trong t∆∞∆°ng lai g·∫ßn) khi·∫øn nhu c·∫ßu:
Ph√¢n t√≠ch d·ªØ li·ªáu th·ªã tr∆∞·ªùng chuy√™n s√¢u
ƒê√°nh gi√° r·ªßi ro ƒë·∫ßu t∆∞
D·ª± ƒëo√°n xu h∆∞·ªõng gi√°
Ki·ªÉm ƒë·ªãnh t√≠nh ·ªïn ƒë·ªãnh c·ªßa t√≠n hi·ªáu giao d·ªãch
tr·ªü n√™n c·∫•p thi·∫øt.
Tuy nhi√™n, ph·∫ßn l·ªõn nh√† ƒë·∫ßu t∆∞ hi·ªán nay:
D·ª±a v√†o c·∫£m t√≠nh ho·∫∑c tin t·ª©c r·ªùi r·∫°c
Thi·∫øu h·ªá th·ªëng ph√¢n t√≠ch d·ªØ li·ªáu c√≥ c·∫•u tr√∫c
Kh√¥ng c√≥ c∆° ch·∫ø ki·ªÉm ch·ª©ng t√≠n hi·ªáu tr∆∞·ªõc khi ra quy·∫øt ƒë·ªãnh
V√¨ v·∫≠y, d·ª± √°n n√†y ƒë∆∞·ª£c x√¢y d·ª±ng nh·∫±m:
X√¢y d·ª±ng m·ªôt h·ªá th·ªëng ph√¢n t√≠ch v√† d·ª± ƒëo√°n t√†i s·∫£n s·ªë d·ª±a tr√™n d·ªØ li·ªáu th·ªùi gian th·ª±c, c√≥ kh·∫£ nƒÉng h·ªó tr·ª£ quy·∫øt ƒë·ªãnh ƒë·∫ßu t∆∞ v·ªõi ƒë·ªô ch√≠nh x√°c t∆∞∆°ng ƒë·ªëi t·ªët, √°p d·ª•ng ƒë∆∞·ª£c cho c·∫£ ng∆∞·ªùi ph√°t tri·ªÉn v√† ng∆∞·ªùi d√πng cu·ªëi (end-user).
H·ªá th·ªëng t·∫≠p trung v√†o c√°c t√†i s·∫£n c√≥ gi√° tr·ªã v√† t√≠nh ·ªïn ƒë·ªãnh cao:
BTC
ETH
BNB

---

# 2Ô∏è‚É£ System Outputs & User Value

This platform delivers multiple layers of value:

## 1. Explainable Trading Signals
- Deterministic BUY / SELL decisions  
- Edge and confidence scoring  
- Transparent metric contribution  
- No black-box logic  

## 2. Controlled Performance Evaluation
- Leakage-safe confirmation framework  
- Adaptive TP/SL risk modeling  
- Expectancy & drawdown analysis  
- Equity curve simulation  

## 3. Structural Market Insights
- Regime-based segmentation  
- FP-Growth structural mining  
- Lift-based validation  
- Recurring condition detection  

## 4. Integrated Market + News Intelligence
- Real-time sentiment ingestion  
- Weighted symbol-level sentiment modeling  
- Window-based aggregation  
- Deterministic integration into scoring  

---

# 3Ô∏è‚É£ System Architecture

![System Architecture](images/System_Architecture.png)

The system follows a layered architecture separating ingestion, transformation, signal modeling, orchestration, validation, and analytics.

---

## 1. Data Ingestion

### Market Data
- Kafka streams real-time OHLCV data  
- Spark Streaming normalizes records  
- Stored in `fact_kline`  

### News Data
- News crawled from crypto media sources  
- Sent to Kafka topic  
- Consumed and stored in `news_fact`  
- Symbol mapping stored in `news_coin_fact`  

Both streams remain independent and immutable.

---

## 2. Indicator Computation

- Atomic indicators computed via Spark  
- Stored in `fact_indicator`  
- Partitioned by symbol and interval  
- Fully recomputable from raw kline  

---

## 3. News Sentiment Processing

News sentiment is modeled as a multi-layer fact pipeline:

### Raw Layer ‚Äî `news_fact`
Grain: 1 row = 1 article  
- title  
- url (UNIQUE)  
- sentiment_score  
- created_date  
- view_number  
- tag_id  

### Mapping Layer ‚Äî `news_coin_fact`
Grain: `(news_id, symbol_id)`  
- symbol attribution  
- confidence score  

### Weighted Layer ‚Äî `news_sentiment_weighted_fact`
Grain: `(news_id, symbol_id)`  

Includes:
- raw_sentiment  
- tag_weight  
- confidence  
- weighted_score  
- final_score  
- event_time  

Constraints:
- UNIQUE(news_id, symbol_id)  
- Indexed for join optimization  

### Aggregated Layer ‚Äî `news_sentiment_agg_fact`
Grain: `(symbol_id, window_start)`  

- news_count  
- sentiment_weighted  

Aligned to trading interval resolution.

---

## 4. Metric Abstraction

- Trading conditions defined in `dim_metric`  
- Technical + sentiment metrics supported  
- Evaluated into `fact_metric_value`  
- Threshold, trend, cross, volatility logic  

---

## 5. Prediction Engine

buy_score  = Œ£(weighted BUY metrics)  
sell_score = Œ£(weighted SELL metrics)  

edge = |buy_score ‚àí sell_score|  
confidence = max(score) / MAX_SCORE  

Stored in `fact_prediction`.

Deterministic, explainable, leakage-safe.

---

## 6. Backtesting & Confirmation

- Adaptive TP/SL  
- Controlled lookahead  
- Results stored in `fact_prediction_result`  
- Strict separation from prediction  

---

## 7. Orchestration Layer (Apache Airflow)

- DAG-based workflow control  
- Spark job scheduling  
- Metric & sentiment pipelines  
- Retry & failure handling  
- CeleryExecutor for distributed tasks  

Runs on Linux-based infrastructure.

---

## 8. Analytics & Pattern Mining

- Win Rate  
- Expectancy  
- Rolling stability  
- Equity curve  
- Drawdown  
- FP-Growth structural validation  

---

# 4Ô∏è‚É£ Data Warehouse Design

![Warehouse ERD](images/warehouse_schema.png)

Fact-driven layered warehouse with explicit grain definition.

## Core Dimensions
- `dim_symbol`
- `dim_interval`
- `dim_indicator_type`
- `dim_metric`

## Market Fact Tables

| Table | Grain | Role |
|-------|-------|------|
| `fact_kline` | (symbol, interval, close_time) | Market data |
| `fact_indicator` | (symbol, interval, indicator, timestamp) | Atomic signals |
| `fact_metric_value` | (symbol, interval, metric, calculating_at) | Conditions |
| `fact_prediction` | (symbol, interval, predicting_at) | Hypothesis |
| `fact_prediction_result` | (prediction_id) | Realized outcome |

## News Fact Tables

| Table | Grain | Role |
|-------|-------|------|
| `news_fact` | (id) | Raw articles |
| `news_coin_fact` | (news_id, symbol_id) | Symbol mapping |
| `news_sentiment_weighted_fact` | (news_id, symbol_id) | Weighted sentiment |
| `news_sentiment_agg_fact` | (symbol_id, window_start) | Aggregated sentiment |

Design Principles:
- Explicit grain  
- Idempotent ETL  
- Event-time alignment  
- No cross-layer coupling  
- Fully traceable lifecycle  

---

# üîü Tech Stack & Engineering Practices

## Tech Stack

- Python  
- PySpark  
- Apache Kafka  
- Apache Airflow (CeleryExecutor)  
- Spark ML (FPGrowth)  
- MySQL 8  
- Flask  
- NumPy / Pandas  

## Deployment Environment

Linux-based infrastructure:

- Kafka service  
- Spark cluster  
- Airflow scheduler + workers  
- MySQL server  
- Flask API  

No containerization.

---

## Engineering Practices

- Layered architecture  
- Explicit grain control  
- Fact-driven warehouse modeling  
- Idempotent ETL  
- UTC normalization  
- Event-time alignment  
- Config-driven strategy logic  
- Strict prediction/confirmation separation  
- DAG-based orchestration  

---

# 11Ô∏è‚É£ Conclusion

This project establishes a deterministic quantitative research infrastructure grounded in structured data modeling and strict leakage control.

By decoupling ingestion (market & news), signal construction, confirmation, and evaluation into independent layers, the system enforces reproducibility, auditability, and disciplined experimentation. Every stage of the signal lifecycle‚Äîtechnical and sentiment‚Äîis persisted at explicit grain within a fact-driven warehouse.

The architecture is designed as a scalable research foundation capable of extending toward:

- Portfolio-level allocation models  
- Transaction cost integration  
- Regime-adaptive weighting  
- Hybrid deterministic‚Äìstatistical extensions  

The objective is not short-term optimization, but the construction of a controlled environment where edge can be measured, validated, and stress-tested under reproducible conditions.

---

